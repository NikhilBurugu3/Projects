{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gesture Recognition\n",
    "In this group project, you are going to build a 3D Conv model that will be able to predict the 5 gestures correctly. Please import the following libraries to get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from scipy.misc import imread, imresize\n",
    "import datetime\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the random seed so that the results don't vary drastically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(30)\n",
    "import random as rn\n",
    "rn.seed(30)\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "#tf.set_random_seed(30)\n",
    "tf.random.set_seed(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this block, you read the folder names for training and validation. You also set the `batch_size` here. Note that you set the batch size in such a way that you are able to use the GPU in full capacity. You keep increasing the batch size until the machine throws an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_doc = np.random.permutation(open('C:/Users/nibu01/Documents/UpGrad/Assignments/Gesture Recognition/Project_data/Project_data/train.csv').readlines())\n",
    "val_doc = np.random.permutation(open('C:/Users/nibu01/Documents/UpGrad/Assignments/Gesture Recognition/Project_data/Project_data/val.csv').readlines())\n",
    "batch_size = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator\n",
    "\n",
    "This is one of the most important part of the code. The overall structure of the generator has been given. In the generator, you are going to preprocess the images as you have images of 2 different dimensions as well as create a batch of video frames. You have to experiment with `img_idx`, `y`,`z` and normalization such that you get high accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Image Parameters\n",
    "img_idx = [2,3,5,7,9,11,13,15,16,17,18,19,20,21,22,23,24,25,26]\n",
    "x = len(img_idx)\n",
    "y=120#image_height\n",
    "z=160#image_width\n",
    "z_crop=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(source_path, folder_list, batch_size):\n",
    "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
    "\n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "        num_batches,partial_batches = divmod(len(folder_list),batch_size)\n",
    "        if num_batches>0:\n",
    "            batch_data, batch_labels = batch_process(x,y,z,num_batches,batch_size,source_path,img_idx,t)\n",
    "            yield batch_data, batch_labels\n",
    "            print(len(batch_labels))\n",
    "        # write the code for the remaining data points which are left after full batches    \n",
    "        elif partial_batches>0:\n",
    "            batch_data, batch_labels = batch_process(x,y,z,partial_batches,batch_size,source_path,img_idx,t)\n",
    "            yield batch_data, batch_labels\n",
    "\n",
    "        \n",
    "        \n",
    "def batch_process(x,y,z,num_batches,batch_size,source_path,img_idx,t):\n",
    "    for batch in range(num_batches): # we iterate over the number of batches\n",
    "            batch_data = np.zeros((batch_size,x,y,z_crop,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    #Resizing the image to 120X160\n",
    "                    #print(\"Before Resizing:::\")\n",
    "                    #print(image.shape)\n",
    "                    image_resized=imresize(image,(y,z,3))\n",
    "                    #print(\"After Resizing:::\")\n",
    "                    #print(image_resized.shape)\n",
    "                    #Cropping the image as the first and last 20 pixels have no useful information\n",
    "                    image_cropped = image_resized[0:120, 20:120]\n",
    "                    #print(\"After Cropping\")\n",
    "                    #print(image_cropped.shape)\n",
    "                    #plt.imshow(cv2.cvtColor(image_cropped, cv2.COLOR_BGR2RGB)) # converting BGR to RGB for using matplotlib\n",
    "                    #plt.show()\n",
    "                    batch_data[folder,idx,:,:,0] = (image_cropped[:,:,0])/255\n",
    "                    batch_data[folder,idx,:,:,1] = (image_cropped[:,:,1])/255\n",
    "                    batch_data[folder,idx,:,:,2] = (image_cropped[:,:,2])/255\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            return (batch_data,batch_labels) #you yield the batch_data and the batch_labels, remember what does yield do\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note here that a video is represented above in the generator as (number of images, height, width, number of channels). Take this into consideration while creating the model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training sequences = 663\n",
      "# validation sequences = 100\n",
      "# epochs = 1\n"
     ]
    }
   ],
   "source": [
    "curr_dt_time = datetime.datetime.now()\n",
    "train_path = 'C:/Users/nibu01/Documents/UpGrad/Assignments/Gesture Recognition/Project_data/Project_data/train'\n",
    "val_path = 'C:/Users/nibu01/Documents/UpGrad/Assignments/Gesture Recognition/Project_data/Project_data/val'\n",
    "num_train_sequences = len(train_doc)\n",
    "print('# training sequences =', num_train_sequences)\n",
    "num_val_sequences = len(val_doc)\n",
    "print('# validation sequences =', num_val_sequences)\n",
    "num_epochs = 1# choose the number of epochs\n",
    "print ('# epochs =', num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_generator = generator(train_path, train_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "#next(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "Here you make the model using different functionalities that Keras provides. Remember to use `Conv3D` and `MaxPooling3D` and not `Conv2D` and `Maxpooling2D` for a 3D convolution model. You would want to use `TimeDistributed` while building a Conv2D + RNN model. Also remember that the last layer is the softmax. Design the network in such a way that the model is able to give good accuracy on the least number of parameters so that it can fit in the memory of the webcam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, GRU, Flatten, TimeDistributed, Flatten, BatchNormalization, Activation,Dropout\n",
    "from keras.layers.convolutional import Conv2D,Conv3D,MaxPooling2D, MaxPooling3D\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import LSTM\n",
    "#write your model here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the frames,No.of rows etc\n",
    "input_shape=(x,y,z_crop,3)\n",
    "featuremap = [8,16,32,64]\n",
    "dense = [128,64,5]\n",
    "classes = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(TimeDistributed(Conv2D(featuremap[0], (3, 3), strides=(2, 2),activation='relu', padding='same'), input_shape=input_shape))\n",
    "model.add(TimeDistributed(Conv2D(featuremap[1], (3,3),padding='same', activation='relu')))\n",
    "model.add(TimeDistributed(MaxPooling2D((2, 2), strides=(2, 2))))\n",
    "\n",
    "model.add(TimeDistributed(Conv2D(featuremap[2], (3,3),padding='same', activation='relu')))\n",
    "model.add(TimeDistributed(MaxPooling2D((2, 2), strides=(2, 2))))\n",
    "\n",
    "model.add(TimeDistributed(Conv2D(featuremap[3], (2,2),padding='same', activation='relu')))\n",
    "model.add(TimeDistributed(MaxPooling2D((2, 2), strides=(2, 2))))\n",
    "\n",
    "model.add(TimeDistributed(BatchNormalization()))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(TimeDistributed(Flatten()))\n",
    "\n",
    "model.add(Dense(dense[0], activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(dense[1], activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(GRU(128, return_sequences=False))\n",
    "model.add(Dense(dense[2], activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have written the model, the next step is to `compile` the model. When you print the `summary` of the model, you'll see the total number of parameters you have to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "time_distributed_64 (TimeDis (None, 19, 60, 50, 8)     224       \n",
      "_________________________________________________________________\n",
      "time_distributed_65 (TimeDis (None, 19, 60, 50, 16)    1168      \n",
      "_________________________________________________________________\n",
      "time_distributed_66 (TimeDis (None, 19, 30, 25, 16)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_67 (TimeDis (None, 19, 30, 25, 32)    4640      \n",
      "_________________________________________________________________\n",
      "time_distributed_68 (TimeDis (None, 19, 15, 12, 32)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_69 (TimeDis (None, 19, 15, 12, 64)    8256      \n",
      "_________________________________________________________________\n",
      "time_distributed_70 (TimeDis (None, 19, 7, 6, 64)      0         \n",
      "_________________________________________________________________\n",
      "time_distributed_71 (TimeDis (None, 19, 7, 6, 64)      256       \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 19, 7, 6, 64)      0         \n",
      "_________________________________________________________________\n",
      "time_distributed_72 (TimeDis (None, 19, 2688)          0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 19, 128)           344192    \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 19, 128)           0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 19, 64)            8256      \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 19, 64)            0         \n",
      "_________________________________________________________________\n",
      "gru_4 (GRU)                  (None, 128)               74496     \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 442,133\n",
      "Trainable params: 442,005\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "optimiser = Adam(0.001)\n",
    "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create the `train_generator` and the `val_generator` which will be used in `.fit_generator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.01, patience=5, cooldown=4, verbose=1,mode='auto',epsilon=0.0001)\n",
    "callbacks_list = [checkpoint, LR]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `steps_per_epoch` and `validation_steps` are used by `fit_generator` to decide the number of next() calls it need to make."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now fit the model. This will start training the model and with the help of the checkpoints, you'll be able to save the model at the end of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  C:/Users/nibu01/Documents/UpGrad/Assignments/Gesture Recognition/Project_data/Project_data/train ; batch size = 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nibu01\\Documents\\Python\\lib\\site-packages\\ipykernel_launcher.py:25: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "C:\\Users\\nibu01\\Documents\\Python\\lib\\site-packages\\ipykernel_launcher.py:31: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      " 1/45 [..............................] - ETA: 0s - loss: 1.6904 - categorical_accuracy: 0.133315\n",
      " 2/45 [>.............................] - ETA: 22s - loss: 1.6785 - categorical_accuracy: 0.200015\n",
      " 3/45 [=>............................] - ETA: 30s - loss: 1.6568 - categorical_accuracy: 0.244415\n",
      " 4/45 [=>............................] - ETA: 31s - loss: 1.6108 - categorical_accuracy: 0.266715\n",
      " 5/45 [==>...........................] - ETA: 36s - loss: 1.6209 - categorical_accuracy: 0.293315\n",
      " 6/45 [===>..........................] - ETA: 39s - loss: 1.5905 - categorical_accuracy: 0.266715\n",
      " 7/45 [===>..........................] - ETA: 41s - loss: 1.5820 - categorical_accuracy: 0.276215\n",
      " 8/45 [====>.........................] - ETA: 38s - loss: 1.5665 - categorical_accuracy: 0.266715\n",
      " 9/45 [=====>........................] - ETA: 39s - loss: 1.5759 - categorical_accuracy: 0.244415\n",
      "10/45 [=====>........................] - ETA: 39s - loss: 1.5526 - categorical_accuracy: 0.253315\n",
      "11/45 [======>.......................] - ETA: 38s - loss: 1.5404 - categorical_accuracy: 0.272715\n",
      "12/45 [=======>......................] - ETA: 38s - loss: 1.5208 - categorical_accuracy: 0.294415\n",
      "13/45 [=======>......................] - ETA: 37s - loss: 1.5177 - categorical_accuracy: 0.307715\n",
      "14/45 [========>.....................] - ETA: 37s - loss: 1.5018 - categorical_accuracy: 0.309515\n",
      "15/45 [=========>....................] - ETA: 36s - loss: 1.5031 - categorical_accuracy: 0.320015\n",
      "16/45 [=========>....................] - ETA: 35s - loss: 1.4977 - categorical_accuracy: 0.320815\n",
      "17/45 [==========>...................] - ETA: 34s - loss: 1.4871 - categorical_accuracy: 0.321615\n",
      "18/45 [===========>..................] - ETA: 34s - loss: 1.4798 - categorical_accuracy: 0.325915\n",
      "19/45 [===========>..................] - ETA: 34s - loss: 1.4602 - categorical_accuracy: 0.336815\n",
      "20/45 [============>.................] - ETA: 33s - loss: 1.4504 - categorical_accuracy: 0.343315\n",
      "21/45 [=============>................] - ETA: 32s - loss: 1.4566 - categorical_accuracy: 0.339715\n",
      "22/45 [=============>................] - ETA: 31s - loss: 1.4463 - categorical_accuracy: 0.348515\n",
      "23/45 [==============>...............] - ETA: 30s - loss: 1.4418 - categorical_accuracy: 0.356515\n",
      "24/45 [===============>..............] - ETA: 29s - loss: 1.4283 - categorical_accuracy: 0.366715\n",
      "25/45 [===============>..............] - ETA: 28s - loss: 1.4107 - categorical_accuracy: 0.378715\n",
      "26/45 [================>.............] - ETA: 27s - loss: 1.4016 - categorical_accuracy: 0.387215\n",
      "27/45 [=================>............] - ETA: 26s - loss: 1.3902 - categorical_accuracy: 0.397515\n",
      "28/45 [=================>............] - ETA: 24s - loss: 1.3829 - categorical_accuracy: 0.402415\n",
      "29/45 [==================>...........] - ETA: 23s - loss: 1.3706 - categorical_accuracy: 0.413815\n",
      "30/45 [===================>..........] - ETA: 22s - loss: 1.3667 - categorical_accuracy: 0.415615\n",
      "31/45 [===================>..........] - ETA: 20s - loss: 1.3550 - categorical_accuracy: 0.421515\n",
      "32/45 [====================>.........] - ETA: 19s - loss: 1.3414 - categorical_accuracy: 0.429215\n",
      "33/45 [=====================>........] - ETA: 17s - loss: 1.3489 - categorical_accuracy: 0.424215\n",
      "34/45 [=====================>........] - ETA: 16s - loss: 1.3482 - categorical_accuracy: 0.427515\n",
      "35/45 [======================>.......] - ETA: 14s - loss: 1.3338 - categorical_accuracy: 0.436215\n",
      "36/45 [=======================>......] - ETA: 13s - loss: 1.3224 - categorical_accuracy: 0.442615\n",
      "37/45 [=======================>......] - ETA: 11s - loss: 1.3292 - categorical_accuracy: 0.443215\n",
      "38/45 [========================>.....] - ETA: 10s - loss: 1.3221 - categorical_accuracy: 0.447415\n",
      "39/45 [=========================>....] - ETA: 8s - loss: 1.3164 - categorical_accuracy: 0.4496 15\n",
      "40/45 [=========================>....] - ETA: 7s - loss: 1.3063 - categorical_accuracy: 0.453315\n",
      "41/45 [==========================>...] - ETA: 6s - loss: 1.3147 - categorical_accuracy: 0.453715\n",
      "42/45 [===========================>..] - ETA: 4s - loss: 1.3006 - categorical_accuracy: 0.458715\n",
      "43/45 [===========================>..] - ETA: 3s - loss: 1.2928 - categorical_accuracy: 0.463615\n",
      "44/45 [============================>.] - ETA: 1s - loss: 1.2815 - categorical_accuracy: 0.468215\n",
      "45/45 [==============================] - ETA: 0s - loss: 1.2801 - categorical_accuracy: 0.4667Source path =  C:/Users/nibu01/Documents/UpGrad/Assignments/Gesture Recognition/Project_data/Project_data/val ; batch size = 15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "\n",
      "Epoch 00001: saving model to model_init_2020-10-3122_36_07.387523\\model-00001-1.28008-0.46667-1.57699-0.31429.h5\n",
      "45/45 [==============================] - 83s 2s/step - loss: 1.2801 - categorical_accuracy: 0.4667 - val_loss: 1.5770 - val_categorical_accuracy: 0.3143\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(history):\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15,4))\n",
    "    axes[0].plot(history.history['loss'])   \n",
    "    axes[0].plot(history.history['val_loss'])\n",
    "    axes[0].legend(['loss','val_loss'])\n",
    "\n",
    "    axes[1].plot(history.history['categorical_accuracy'])   \n",
    "    axes[1].plot(history.history['val_categorical_accuracy'])\n",
    "    axes[1].legend(['categorical_accuracy','val_categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm = Sequential()\n",
    "model_lstm.add(LSTM(2048, return_sequences=False,input_shape=(1000,5),dropout=0.5))\n",
    "model_lstm.add(Dense(512, activation='relu'))\n",
    "model_lstm.add(Dropout(0.5))\n",
    "model_lstm.add(Dense(5, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_4 (LSTM)                (None, 2048)              16826368  \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 512)               1049088   \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 5)                 2565      \n",
      "=================================================================\n",
      "Total params: 17,878,021\n",
      "Trainable params: 17,878,021\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "optimiser = Adam(0.001)\n",
    "model_lstm.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model_lstm.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nibu01\\Documents\\Python\\lib\\site-packages\\ipykernel_launcher.py:25: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "C:\\Users\\nibu01\\Documents\\Python\\lib\\site-packages\\ipykernel_launcher.py:31: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      " 1/45 [..............................] - ETA: 0s - loss: 1.0334 - categorical_accuracy: 0.600015\n",
      " 2/45 [>.............................] - ETA: 23s - loss: 1.0078 - categorical_accuracy: 0.600015\n",
      " 3/45 [=>............................] - ETA: 33s - loss: 0.9232 - categorical_accuracy: 0.622215\n",
      " 4/45 [=>............................] - ETA: 38s - loss: 0.8750 - categorical_accuracy: 0.650015\n",
      " 5/45 [==>...........................] - ETA: 41s - loss: 0.9199 - categorical_accuracy: 0.600015\n",
      " 6/45 [===>..........................] - ETA: 41s - loss: 0.9533 - categorical_accuracy: 0.588915\n",
      " 7/45 [===>..........................] - ETA: 39s - loss: 0.9126 - categorical_accuracy: 0.609515\n",
      " 8/45 [====>.........................] - ETA: 39s - loss: 0.9671 - categorical_accuracy: 0.600015\n",
      " 9/45 [=====>........................] - ETA: 37s - loss: 0.9447 - categorical_accuracy: 0.607415\n",
      "10/45 [=====>........................] - ETA: 37s - loss: 0.9798 - categorical_accuracy: 0.586715\n",
      "11/45 [======>.......................] - ETA: 39s - loss: 0.9618 - categorical_accuracy: 0.600015\n",
      "12/45 [=======>......................] - ETA: 38s - loss: 0.9393 - categorical_accuracy: 0.616715\n",
      "13/45 [=======>......................] - ETA: 37s - loss: 0.9251 - categorical_accuracy: 0.620515\n",
      "14/45 [========>.....................] - ETA: 37s - loss: 0.9163 - categorical_accuracy: 0.623815\n",
      "15/45 [=========>....................] - ETA: 37s - loss: 0.9216 - categorical_accuracy: 0.631115\n",
      "16/45 [=========>....................] - ETA: 35s - loss: 0.8997 - categorical_accuracy: 0.650015\n",
      "17/45 [==========>...................] - ETA: 35s - loss: 0.8871 - categorical_accuracy: 0.651015\n",
      "18/45 [===========>..................] - ETA: 33s - loss: 0.8812 - categorical_accuracy: 0.648115\n",
      "19/45 [===========>..................] - ETA: 33s - loss: 0.8754 - categorical_accuracy: 0.656115\n",
      "20/45 [============>.................] - ETA: 32s - loss: 0.8675 - categorical_accuracy: 0.656715\n",
      "21/45 [=============>................] - ETA: 32s - loss: 0.8666 - categorical_accuracy: 0.657115\n",
      "22/45 [=============>................] - ETA: 31s - loss: 0.8562 - categorical_accuracy: 0.657615\n",
      "23/45 [==============>...............] - ETA: 30s - loss: 0.8465 - categorical_accuracy: 0.666715\n",
      "24/45 [===============>..............] - ETA: 29s - loss: 0.8267 - categorical_accuracy: 0.675015\n",
      "25/45 [===============>..............] - ETA: 28s - loss: 0.8433 - categorical_accuracy: 0.664015\n",
      "26/45 [================>.............] - ETA: 26s - loss: 0.8397 - categorical_accuracy: 0.664115\n",
      "27/45 [=================>............] - ETA: 25s - loss: 0.8304 - categorical_accuracy: 0.666715\n",
      "28/45 [=================>............] - ETA: 24s - loss: 0.8279 - categorical_accuracy: 0.669015\n",
      "29/45 [==================>...........] - ETA: 22s - loss: 0.8247 - categorical_accuracy: 0.669015\n",
      "30/45 [===================>..........] - ETA: 21s - loss: 0.8157 - categorical_accuracy: 0.673315\n",
      "31/45 [===================>..........] - ETA: 19s - loss: 0.8085 - categorical_accuracy: 0.675315\n",
      "32/45 [====================>.........] - ETA: 18s - loss: 0.8037 - categorical_accuracy: 0.677115\n",
      "33/45 [=====================>........] - ETA: 17s - loss: 0.7984 - categorical_accuracy: 0.680815\n",
      "34/45 [=====================>........] - ETA: 15s - loss: 0.8025 - categorical_accuracy: 0.680415\n",
      "35/45 [======================>.......] - ETA: 14s - loss: 0.8058 - categorical_accuracy: 0.678115\n",
      "36/45 [=======================>......] - ETA: 13s - loss: 0.8056 - categorical_accuracy: 0.681515\n",
      "37/45 [=======================>......] - ETA: 11s - loss: 0.8004 - categorical_accuracy: 0.686515\n",
      "38/45 [========================>.....] - ETA: 10s - loss: 0.7987 - categorical_accuracy: 0.687715\n",
      "39/45 [=========================>....] - ETA: 8s - loss: 0.8008 - categorical_accuracy: 0.6838 15\n",
      "40/45 [=========================>....] - ETA: 7s - loss: 0.8043 - categorical_accuracy: 0.676715\n",
      "41/45 [==========================>...] - ETA: 5s - loss: 0.7948 - categorical_accuracy: 0.679715\n",
      "42/45 [===========================>..] - ETA: 4s - loss: 0.7904 - categorical_accuracy: 0.681015\n",
      "43/45 [===========================>..] - ETA: 3s - loss: 0.7834 - categorical_accuracy: 0.683715\n",
      "44/45 [============================>.] - ETA: 1s - loss: 0.7839 - categorical_accuracy: 0.684815\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.7808 - categorical_accuracy: 0.685915\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "\n",
      "Epoch 00001: saving model to model_init_2020-10-3122_36_07.387523\\model-00001-0.78084-0.68593-1.51670-0.39048.h5\n",
      "45/45 [==============================] - 81s 2s/step - loss: 0.7808 - categorical_accuracy: 0.6859 - val_loss: 1.5167 - val_categorical_accuracy: 0.3905\n"
     ]
    }
   ],
   "source": [
    "lstm_model = model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
